<div align="center">

# AI Voice Agent (30â€‘Day Build)

[![Python 3.11+](https://img.shields.io/badge/python-3.11%2B-blue.svg)](https://www.python.org/)  
[![FastAPI](https://img.shields.io/badge/FastAPI-0.111-009688?logo=fastapi)](https://fastapi.tiangolo.com/)
[![AssemblyAI](https://img.shields.io/badge/STT-AssemblyAI-5932F3)](https://www.assemblyai.com/)
[![Gemini LLM](https://img.shields.io/badge/LLM-Gemini-4285F4)](https://ai.google.dev/)
[![Murf AI](https://img.shields.io/badge/TTS-Murf.ai-FF8800)](https://murf.ai/)

Natural, voiceâ€‘first conversational AI: Speak â†’ Transcribe (AssemblyAI) â†’ Reason (Gemini) â†’ Respond with realistic speech (Murf)

</div>

<p align="center">
  <img src="app/static/images/demo.gif" alt="Demo Conversation" />
</p>

## âœ¨ Core Features

- Oneâ€‘tap voice chat (microphone â†’ AI answer with autoâ€‘played voice)
- Multiâ€‘stage pipeline: STT â†’ LLM â†’ TTS
- Persistent inâ€‘memory session history (per browser session id)
- Sidebar Tools:
  - Text to Speech generator (choose text â†’ Murf voice output)
  - Echo Bot (record â†’ transcribe â†’ reâ€‘speak your words in another voice)
- Replay last AI audio + chat bubble UI
- Clean, minimal vanilla JS frontend (no heavy frameworks)

## ğŸ§  Architecture Flow

1. User presses Start Speaking â†’ Browser records audio (MediaRecorder)
2. Audio uploaded to `/agent/chat/{session_id}`
3. AssemblyAI transcribes bytes â†’ text
4. Chat history compiled into a Gemini prompt
5. Gemini generates assistant reply
6. Murf API converts reply text to speech (voice: en-US-ken)
7. Frontend autoâ€‘plays the returned audio & renders chat bubbles

```
User Voice â†’ FastAPI â†’ AssemblyAI â†’ Gemini â†’ Murf â†’ Browser Playback
```

## ğŸ—‚ï¸ Project Structure

```
app/
â”œâ”€â”€ main.py                # FastAPI entrypoint (routes import service layer)
â”œâ”€â”€ services/              # Separated domain/service logic
â”‚   â”œâ”€â”€ stt_service.py     # AssemblyAI transcription helpers
â”‚   â”œâ”€â”€ tts_service.py     # Murf.ai TTS client wrapper
â”‚   â””â”€â”€ llm_service.py     # Gemini client + prompt builder
â”œâ”€â”€ schemas/               # Pydantic request/response models
â”‚   â””â”€â”€ tts.py             # TextToSpeechRequest, ChatResponse, etc.
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html         # UI shell (chat + sidebar tools)
â”œâ”€â”€ static/
â”‚   â”œâ”€â”€ css/style.css      # Styles (layout + responsive + theme)
â”‚   â”œâ”€â”€ JS/script.js       # Frontend logic (record, upload, autoplay)
â”‚   â”œâ”€â”€ images/            # Logo, screenshot, demo GIF
â”‚   â”‚   â”œâ”€â”€ logo.png
â”‚   â”‚   â”œâ”€â”€ ui-screenshot.png
â”‚   â”‚   â””â”€â”€ demo.gif
â”‚   â””â”€â”€ sounds/            # Mic UI feedback
â”‚       â”œâ”€â”€ mic_start.mp3
â”‚       â””â”€â”€ mic_stop.mp3
â”œâ”€â”€ uploads/               # (Optional) temp upload storage placeholder
requirements.txt           # Dependencies
.env                       # Secret API keys (NOT committed)
.gitignore                 # Ignore rules
README.md                  # This file
```

## ğŸ”‘ Environment Variables (.env)

Create a `.env` file in the project root:

```
ASSEMBLYAI_API_KEY=your_assemblyai_key
GEMINI_API_KEY=your_gemini_key
MURF_API_KEY=your_murf_key
```

## ğŸš€ Quick Start

```bash
# 1. Create & activate a virtual environment
python -m venv .venv
.venv\Scripts\activate  # Windows

# 2. Install dependencies
pip install -r requirements.txt

# 3. Add your .env file (see above)

# 4. Run the server (simple dev mode)
cd app && python main.py

# 5. Open in browser
http://127.0.0.1:8000/

# (Alt) Use uvicorn directly for auto-reload (optional)
# uvicorn app.main:app --reload
```

## ğŸ“¡ Key Endpoints

| Method | Endpoint                   | Purpose                                       |
| ------ | -------------------------- | --------------------------------------------- |
| POST   | `/agent/chat/{session_id}` | Voice chat: audio â†’ transcription â†’ LLM â†’ TTS |
| POST   | `/tts/echo`                | Echo tool (repeat what you said with Murf)    |
| POST   | `/generate_audio`          | Direct text â†’ speech (Murf)                   |
| POST   | `/transcribe/file`         | Raw transcription (AssemblyAI)                |

## ğŸ§ª Tech Highlights

- FastAPI backend with service + schema layering (clean separation)
- AssemblyAI transcription (resilient + fallback path)
- Google Gemini (gemini-1.5-flash) via reusable client & retry logic
- Murf AI TTS wrapped in a lightweight client (consistent error handling)
- MediaRecorder + multipart upload for low-latency voice capture
- Autoplay + replay logic with audio unlock and retry
- Structured Pydantic responses for clearer API contracts

## ğŸ”„ Session Handling

Browser session id is appended to the URL (query param). History is stored in an inâ€‘memory dict (`CHAT_HISTORY`) â€” suitable for prototyping; swap with Redis or DB for production scaling.

## ğŸ›¡ï¸ Notes / Limits

- Not production-hardened (no auth, rate limiting, or persistence yet)
- API keys must remain secret (.env not committed)
- In-memory history resets on server restart (swap with Redis/DB later)
- Gemini key must be loaded before first request (lazy reconfigure added)

## ğŸ¤ Contributing

Prototype phase â€” feel free to open issues with ideas (latency, UI/UX, voice packs, multilingual support). PRs welcome after discussion.

<!--## ğŸ“„ License

Add a LICENSE file (MIT recommended) if you plan to open source formally.-->

## ğŸ™Œ Acknowledgements

- AssemblyAI for speech-to-text
- Google Gemini for language understanding
- Murf AI for high-quality synthetic voices
- FastAPI for the rapid backend framework

---

Built as part of a 30â€‘Day AI Voice Agent Challenge by <a href="https://murf.ai/" target="_blank">Murf.ai</a>
